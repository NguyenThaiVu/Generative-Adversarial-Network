{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Semi_supervised_GAN.ipynb","provenance":[],"authorship_tag":"ABX9TyOT6NimC+ASfSBiM//LcY2+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"kX6RAir5qM_l","executionInfo":{"status":"ok","timestamp":1625542962578,"user_tz":-420,"elapsed":2316,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras import backend\n","\n","import numpy as np\n","\n","from sklearn.utils import shuffle\n","from imutils import build_montages\n","import numpy as np\n","import argparse\n","import cv2\n","import os"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2_MTCdxstC_"},"source":["# Define discriminator and generator"]},{"cell_type":"code","metadata":{"id":"iFERAVrArrYK","executionInfo":{"status":"ok","timestamp":1625542962579,"user_tz":-420,"elapsed":13,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["def custom_activation(output):\n","\tlogexpsum = backend.sum(backend.exp(output), axis=-1, keepdims=True)\n","\tresult = logexpsum / (logexpsum + 1.0)\n","\treturn result\n","\n","def define_discriminator(in_shape=(28,28,1), n_classes=10):\n","\tin_image = Input(shape=in_shape)\n"," \n","\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(in_image)\n","\tfe = LeakyReLU(alpha=0.2)(fe)\n","\n","\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n","\tfe = LeakyReLU(alpha=0.2)(fe)\n","\n","\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n","\tfe = LeakyReLU(alpha=0.2)(fe)\n","\n","\tfe = Flatten()(fe)\n","\tfe = Dropout(0.4)(fe)\n","\tfe = Dense(n_classes)(fe)\n","\n","\tc_out_layer = Activation('softmax')(fe)\n","\tc_model = Model(in_image, c_out_layer)\n","\tc_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n","\n","\td_out_layer = Lambda(custom_activation)(fe)\n","\td_model = Model(in_image, d_out_layer)\n","\td_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n","\n","\treturn d_model, c_model"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hjJCU4nr2HI","executionInfo":{"status":"ok","timestamp":1625542962580,"user_tz":-420,"elapsed":13,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["def define_generator(latent_dim):\n","\n","\tin_lat = Input(shape=(latent_dim,))\n","\n","\tn_nodes = 128 * 7 * 7\n","\n","\tgen = Dense(n_nodes)(in_lat)\n","\tgen = LeakyReLU(alpha=0.2)(gen)\n","\tgen = Reshape((7, 7, 128))(gen)\n","\n","\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n","\tgen = LeakyReLU(alpha=0.2)(gen)\n","\n","\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n","\tgen = LeakyReLU(alpha=0.2)(gen)\n","\n","\tout_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n","\n","\tmodel = Model(in_lat, out_layer)\n","\n","\treturn model"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDRg4Ft6sEsp","executionInfo":{"status":"ok","timestamp":1625542962581,"user_tz":-420,"elapsed":13,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["def define_gan(g_model, d_model):\n","\n","\td_model.trainable = False\n","\n","\tgan_output = d_model(g_model.output)\n","\n","\tmodel = Model(g_model.input, gan_output)\n","\n","\topt = Adam(learning_rate=0.0002, beta_1=0.5)\n","\n","\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n","\treturn model"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D6RtLfbZsvue"},"source":["# Tranining Semi-supervised GAN"]},{"cell_type":"code","metadata":{"id":"ZgOVbbh3sYCi","executionInfo":{"status":"ok","timestamp":1625542962582,"user_tz":-420,"elapsed":14,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["from tensorflow.keras.datasets.mnist import load_data\n","\n","def load_real_samples():\n","\n","\t(trainX, trainy), (_, _) = load_data()\n","\n","\tX = np.expand_dims(trainX, axis=-1)\n","\n","\tX = X.astype('float32')\n","\n","\tX = (X - 127.5) / 127.5\n","\n","\tprint(X.shape, trainy.shape)\n","\treturn [X, trainy]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"apQRMTyJs62j","executionInfo":{"status":"ok","timestamp":1625542962583,"user_tz":-420,"elapsed":14,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["# select a supervised subset of the dataset, ensures classes are balanced\n","def select_supervised_samples(dataset, n_samples=100, n_classes=10):\n","\t\n","    X, y = dataset\n","    X_list, y_list = list(), list()\n","    n_per_class = int(n_samples / n_classes)\n","\t\n","    for i in range(n_classes):\n","\n","        X_with_class = X[y == i]\n","\n","        ix = np.random.randint(0, len(X_with_class), n_per_class)\n","\n","        [X_list.append(X_with_class[j]) for j in ix]\n","        [y_list.append(i) for j in ix]\n","\n","    return np.asarray(X_list), np.asarray(y_list)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVsFY8zGtTAH","executionInfo":{"status":"ok","timestamp":1625542962583,"user_tz":-420,"elapsed":13,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["# Function select real samples from dataset\n","def generate_real_samples(dataset, n_samples):\n","\n","\timages, labels = dataset\n","\n","\tix = np.random.randint(0, images.shape[0], n_samples)\n","\n","\tX, labels = images[ix], labels[ix]\n","\ty = np.ones((n_samples, 1))\n","\n","\treturn [X, labels], y"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"cl_mZIZ4tiaO","executionInfo":{"status":"ok","timestamp":1625542962584,"user_tz":-420,"elapsed":14,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","\t\n","    gen_input = np.random.randn(latent_dim * n_samples)\n","    gen_input = gen_input.reshape(n_samples, latent_dim)\n","\n","    images = generator.predict(gen_input)\n","\n","    y = np.zeros((n_samples, 1))\n","    return images, y"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MPGSbr6wv9q2","executionInfo":{"status":"ok","timestamp":1625542962584,"user_tz":-420,"elapsed":14,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}},"outputId":"73a0a094-2448-4961-b9d7-472762056a79"},"source":["OUTPUT_PATH = \"output\"\n","\n","! mkdir \"output\""],"execution_count":9,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘output’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1qxa2lK_uaLh","executionInfo":{"status":"ok","timestamp":1625543530823,"user_tz":-420,"elapsed":578,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["# train the generator and discriminator\n","def train(g_model, d_model, c_model, gan_model, dataset, latent_dim, n_epochs=20, batch_size=100):\n","\n","    X_sup, y_sup = select_supervised_samples(dataset)\n","    print(X_sup.shape, y_sup.shape)\n","\n","    bat_per_epo = int(dataset[0].shape[0] / batch_size)\n","    n_steps = bat_per_epo * n_epochs\n","    half_batch = int(batch_size / 2)\n","    print('n_epochs = %d, batch_size = %d, 1/2 = %d, b/e = %d, steps = %d' % (n_epochs, batch_size, half_batch, bat_per_epo, n_steps))\n","\n","    for i in range(n_steps):\n","\n","        # update supervised discriminator (c)\n","        [Xsup_real, ysup_real], _ = generate_real_samples([X_sup, y_sup], half_batch)\n","        c_loss, c_acc = c_model.train_on_batch(Xsup_real, ysup_real)\n","\n","        # update unsupervised discriminator (d)\n","        [X_real, _], y_real = generate_real_samples(dataset, half_batch)\n","        d_loss1 = d_model.train_on_batch(X_real, y_real)\n","        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","        d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n","\n","        # update generator (g)\n","        X_gan = np.random.randn(latent_dim * batch_size)\n","        X_gan = X_gan.reshape(batch_size, latent_dim)\n","        y_gan = np.ones((batch_size, 1))\n","\n","        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n","\n","        # evaluate the model performance every so often\n","        if (i+1) % (bat_per_epo * 1) == 0:\n","\n","            epoch = (i + 1) / bat_per_epo\n","\n","            # summarize loss on this epoch\n","            print('[INFO] %d, c[%.3f,%.0f], d[%.3f,%.3f], g[%.3f]' % (epoch, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n","            \n","            p = [OUTPUT_PATH, \"epoch_{}_output.png\".format(str(epoch))]\n","\n","            images, _ = generate_fake_samples(g_model, latent_dim, 64)\n","\n","            images = ((images * 127.5) + 127.5).astype(\"uint8\")\n","            images = np.repeat(images, 3, axis=-1)\n","            vis = build_montages(images, (28, 28), (8, 8))[0]\n","\n","            # write the visualization to disk\n","            p = os.path.sep.join(p)\n","            print(\"path: \", p)\n","            cv2.imwrite(p, vis)\n","\n","            # evaluate the classifier model\n","            X, y = dataset\n","            _, acc = c_model.evaluate(X, y, verbose=0)\n","            print('Classifier Accuracy: %.3f%%' % (acc * 100))\n","\n","            # save the generator model\n","            filename2 = 'g_model_%04d.h5' % (i+1)\n","            g_model.save(filename2)\n","            \n","            # save the classifier model\n","            filename3 = 'c_model_%04d.h5' % (i+1)\n","            c_model.save(filename3)\n","            print('>Saved: %s, and %s' % (filename2, filename3))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WI0oplYsvzuz","executionInfo":{"status":"ok","timestamp":1625544556319,"user_tz":-420,"elapsed":1025498,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}},"outputId":"b60354eb-4211-436c-f73e-728472a69084"},"source":["latent_dim = 100\n","\n","d_model, c_model = define_discriminator()\n","\n","g_model = define_generator(latent_dim)\n","\n","gan_model = define_gan(g_model, d_model)\n","\n","dataset = load_real_samples()\n","\n","train(g_model, d_model, c_model, gan_model, dataset, latent_dim)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["(60000, 28, 28, 1) (60000,)\n","(100, 28, 28, 1) (100,)\n","n_epochs = 20, batch_size = 100, 1/2 = 50, b/e = 600, steps = 12000\n","[INFO] 1, c[0.032,100], d[0.606,0.577], g[1.549]\n","path:  output/epoch_01.0_output.png\n","Classifier Accuracy: 83.025%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_0600.h5, and c_model_0600.h5\n","[INFO] 2, c[0.013,100], d[0.563,0.663], g[1.482]\n","path:  output/epoch_02.0_output.png\n","Classifier Accuracy: 89.332%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_1200.h5, and c_model_1200.h5\n","[INFO] 3, c[0.019,100], d[0.652,0.603], g[1.500]\n","path:  output/epoch_03.0_output.png\n","Classifier Accuracy: 91.265%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_1800.h5, and c_model_1800.h5\n","[INFO] 4, c[0.006,100], d[0.741,0.630], g[1.126]\n","path:  output/epoch_04.0_output.png\n","Classifier Accuracy: 93.453%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_2400.h5, and c_model_2400.h5\n","[INFO] 5, c[0.005,100], d[0.720,0.825], g[1.324]\n","path:  output/epoch_05.0_output.png\n","Classifier Accuracy: 93.792%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_3000.h5, and c_model_3000.h5\n","[INFO] 6, c[0.004,100], d[0.701,0.674], g[1.281]\n","path:  output/epoch_06.0_output.png\n","Classifier Accuracy: 94.377%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_3600.h5, and c_model_3600.h5\n","[INFO] 7, c[0.005,100], d[0.606,0.761], g[1.273]\n","path:  output/epoch_07.0_output.png\n","Classifier Accuracy: 94.100%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_4200.h5, and c_model_4200.h5\n","[INFO] 8, c[0.002,100], d[0.818,0.618], g[1.130]\n","path:  output/epoch_08.0_output.png\n","Classifier Accuracy: 94.983%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_4800.h5, and c_model_4800.h5\n","[INFO] 9, c[0.002,100], d[0.705,0.936], g[1.155]\n","path:  output/epoch_09.0_output.png\n","Classifier Accuracy: 94.742%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_5400.h5, and c_model_5400.h5\n","[INFO] 10, c[0.001,100], d[0.877,0.915], g[1.240]\n","path:  output/epoch_10.0_output.png\n","Classifier Accuracy: 94.910%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_6000.h5, and c_model_6000.h5\n","[INFO] 11, c[0.002,100], d[0.923,0.615], g[1.128]\n","path:  output/epoch_11.0_output.png\n","Classifier Accuracy: 94.503%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_6600.h5, and c_model_6600.h5\n","[INFO] 12, c[0.001,100], d[0.575,0.794], g[1.069]\n","path:  output/epoch_12.0_output.png\n","Classifier Accuracy: 95.105%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_7200.h5, and c_model_7200.h5\n","[INFO] 13, c[0.002,100], d[0.778,0.803], g[1.239]\n","path:  output/epoch_13.0_output.png\n","Classifier Accuracy: 95.408%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_7800.h5, and c_model_7800.h5\n","[INFO] 14, c[0.001,100], d[0.773,0.732], g[1.068]\n","path:  output/epoch_14.0_output.png\n","Classifier Accuracy: 94.568%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_8400.h5, and c_model_8400.h5\n","[INFO] 15, c[0.000,100], d[0.933,0.777], g[1.261]\n","path:  output/epoch_15.0_output.png\n","Classifier Accuracy: 94.842%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_9000.h5, and c_model_9000.h5\n","[INFO] 16, c[0.001,100], d[0.631,0.746], g[1.142]\n","path:  output/epoch_16.0_output.png\n","Classifier Accuracy: 94.145%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_9600.h5, and c_model_9600.h5\n","[INFO] 17, c[0.001,100], d[0.885,0.867], g[1.264]\n","path:  output/epoch_17.0_output.png\n","Classifier Accuracy: 93.282%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_10200.h5, and c_model_10200.h5\n","[INFO] 18, c[0.001,100], d[0.875,0.973], g[1.019]\n","path:  output/epoch_18.0_output.png\n","Classifier Accuracy: 93.070%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_10800.h5, and c_model_10800.h5\n","[INFO] 19, c[0.001,100], d[1.130,1.178], g[1.185]\n","path:  output/epoch_19.0_output.png\n","Classifier Accuracy: 91.775%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_11400.h5, and c_model_11400.h5\n","[INFO] 20, c[0.001,100], d[0.653,0.781], g[1.131]\n","path:  output/epoch_20.0_output.png\n","Classifier Accuracy: 91.628%\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",">Saved: g_model_12000.h5, and c_model_12000.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XROVRgXSwYnm","executionInfo":{"status":"aborted","timestamp":1625543017223,"user_tz":-420,"elapsed":5,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":[""],"execution_count":null,"outputs":[]}]}