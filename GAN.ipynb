{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN.ipynb","provenance":[],"authorship_tag":"ABX9TyPqMFEB2IDUE4u0TxBehTH/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JxXBxYJMgZJF"},"source":["- Notebook demonstrate how to implement GAN from scratch using Keras, tensorflow. \n","- Using GAN to generate digit image (learning from MNIST dataset)."]},{"cell_type":"code","metadata":{"id":"Uy5IKfMAWNFA","executionInfo":{"status":"ok","timestamp":1625502886220,"user_tz":-420,"elapsed":1696,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import *"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kSNPQ5V_Y4Ph"},"source":["# Implement GAN"]},{"cell_type":"code","metadata":{"id":"LGXhfMhiWhgX","executionInfo":{"status":"ok","timestamp":1625502886221,"user_tz":-420,"elapsed":5,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["class DCGAN:\n","\n","    @staticmethod\n","    def build_generator(dim, depth, channels=1, inputDim=100, outputDim=512):\n","\n","        model = Sequential()\n","        inputShape = (dim, dim, depth)\n","        chanDim = -1\n","\n","        model.add(Dense(input_dim=inputDim, units=outputDim))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization())\n","\n","        model.add(Dense(dim * dim * depth))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization())\n","\n","        model.add(Reshape(inputShape))\n","\n","        model.add(Conv2DTranspose(32, (5, 5), strides=(2, 2), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","\n","        model.add(Conv2DTranspose(channels, (5, 5), strides=(2, 2), padding=\"same\"))\n","        model.add(Activation(\"tanh\"))\n","        \n","        return model\n","\n","    @staticmethod\n","    def build_discriminator(width, height, depth, alpha=0.2):\n","\n","        model = Sequential()\n","\n","        inputShape = (height, width, depth)\n","\n","        model.add(Conv2D(32, (5, 5), padding=\"same\", strides=(2, 2), input_shape=inputShape))\n","        model.add(LeakyReLU(alpha=alpha))\n","\n","        model.add(Conv2D(64, (5, 5), padding=\"same\", strides=(2, 2)))\n","        model.add(LeakyReLU(alpha=alpha))\n","\n","        model.add(Flatten())\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=alpha))\n","\n","        model.add(Dense(1))\n","        model.add(Activation(\"sigmoid\"))\n","\n","        return model"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzeXfgGGaD7Z"},"source":["# Training GAN"]},{"cell_type":"code","metadata":{"id":"yRXUQd80ZinG","executionInfo":{"status":"ok","timestamp":1625502886221,"user_tz":-420,"elapsed":4,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.datasets import fashion_mnist, mnist\n","from sklearn.utils import shuffle\n","from imutils import build_montages\n","import numpy as np\n","import argparse\n","import cv2\n","import os"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cglbJnaaI78","executionInfo":{"status":"ok","timestamp":1625502886741,"user_tz":-420,"elapsed":523,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}},"outputId":"b7494bff-eb15-49cf-8261-3db164916dff"},"source":["OUTPUT_PATH = \"output\"\n","NUM_EPOCHS = 20\n","BATCH_SIZE = 128\n","INIT_LR = 2e-4\n","\n","! mkdir \"output\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘output’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FdfnG0yvaRQN","executionInfo":{"status":"ok","timestamp":1625502887215,"user_tz":-420,"elapsed":476,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["# Load MNIST dataset\n","\n","((trainX, _), (testX, _)) = mnist.load_data()\n","trainImages = np.concatenate([trainX, testX])\n","\n","trainImages = np.expand_dims(trainImages, axis=-1)\n","\n","trainImages = (trainImages.astype(\"float\") - 127.5) / 127.5"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESVgasoRafu3","executionInfo":{"status":"ok","timestamp":1625502887720,"user_tz":-420,"elapsed":506,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["# Build generator model\n","gen = DCGAN.build_generator(7, 64, channels=1)\n","\n","# Build discriminator model\n","disc = DCGAN.build_discriminator(28, 28, 1)\n","discOpt = Adam(learning_rate=INIT_LR, beta_1=0.5, decay=INIT_LR / NUM_EPOCHS)\n","disc.compile(loss=\"binary_crossentropy\", optimizer=discOpt)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"WUGxBt-Cau31","executionInfo":{"status":"ok","timestamp":1625502887721,"user_tz":-420,"elapsed":3,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":["# Build GAN\n","disc.trainable = False  # FREEZE PARAMETERE\n","ganInput = Input(shape=(100,))\n","ganOutput = disc(gen(ganInput))\n","gan = Model(ganInput, ganOutput)\n","\n","# compile the GAN\n","ganOpt = Adam(learning_rate=INIT_LR, beta_1=0.5, decay=INIT_LR / NUM_EPOCHS)\n","gan.compile(loss=\"binary_crossentropy\", optimizer=discOpt)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQrHQPfibCkL","executionInfo":{"status":"ok","timestamp":1625503417369,"user_tz":-420,"elapsed":529650,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}},"outputId":"0a3ce3b9-25c6-4cc9-d615-06d5c4243212"},"source":["# Training process\n","benchmarkNoise = np.random.uniform(-1, 1, size=(64, 100))\n","\n","# loop over the epochs\n","for epoch in range(0, NUM_EPOCHS):\n","    print(\"[INFO] starting epoch {} of {}...\".format(epoch + 1, NUM_EPOCHS))\n","    batchesPerEpoch = int(trainImages.shape[0] / BATCH_SIZE)\n","\n","    for i in range(0, batchesPerEpoch):\n","        \n","        p = None\n","        imageBatch = trainImages[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n","        noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n","\n","        # Generator generate image\n","        genImages = gen.predict(noise, verbose=0)\n","\n","        # Concat \"real\" images and \"'fake\" images\n","        X = np.concatenate((imageBatch, genImages))\n","        y = ([1] * BATCH_SIZE) + ([0] * BATCH_SIZE)\n","        y = np.reshape(y, (-1,))\n","        (X, y) = shuffle(X, y)\n","\n","        # train the discriminator on the data\n","        discLoss = disc.train_on_batch(X, y)\n","\n","        # Inverted label\n","        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n","        fakeLabels = [1] * BATCH_SIZE\n","        fakeLabels = np.reshape(fakeLabels, (-1,))\n","        ganLoss = gan.train_on_batch(noise, fakeLabels)\n","\n","        # End of Epoch\n","        if i == batchesPerEpoch - 1:\n","\t\t\t\n","            p = [OUTPUT_PATH, \"epoch_{}_output.png\".format(str(epoch + 1).zfill(4))]\n","\n","            print(\"[INFO] Step {}_{}: discriminator_loss={:.6f}, \"\"adversarial_loss={:.6f}\".format(epoch + 1, i, discLoss, ganLoss))\n","\n","            # make predictions on the benchmark noise, scale it back to [0, 255], and generate the montage\n","            images = gen.predict(benchmarkNoise)\n","            images = ((images * 127.5) + 127.5).astype(\"uint8\")\n","            images = np.repeat(images, 3, axis=-1)\n","            vis = build_montages(images, (28, 28), (8, 8))[0]\n","\n","            # write the visualization to disk\n","            p = os.path.sep.join(p)\n","            print(\"path: \", p)\n","            cv2.imwrite(p, vis)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[INFO] starting epoch 1 of 20...\n","[INFO] Step 1_545: discriminator_loss=0.297950, adversarial_loss=1.609284\n","path:  output/epoch_0001_output.png\n","[INFO] starting epoch 2 of 20...\n","[INFO] Step 2_545: discriminator_loss=0.453995, adversarial_loss=1.509171\n","path:  output/epoch_0002_output.png\n","[INFO] starting epoch 3 of 20...\n","[INFO] Step 3_545: discriminator_loss=0.492564, adversarial_loss=1.685188\n","path:  output/epoch_0003_output.png\n","[INFO] starting epoch 4 of 20...\n","[INFO] Step 4_545: discriminator_loss=0.504698, adversarial_loss=1.195726\n","path:  output/epoch_0004_output.png\n","[INFO] starting epoch 5 of 20...\n","[INFO] Step 5_545: discriminator_loss=0.500017, adversarial_loss=1.659174\n","path:  output/epoch_0005_output.png\n","[INFO] starting epoch 6 of 20...\n","[INFO] Step 6_545: discriminator_loss=0.440981, adversarial_loss=1.431539\n","path:  output/epoch_0006_output.png\n","[INFO] starting epoch 7 of 20...\n","[INFO] Step 7_545: discriminator_loss=0.472230, adversarial_loss=1.475965\n","path:  output/epoch_0007_output.png\n","[INFO] starting epoch 8 of 20...\n","[INFO] Step 8_545: discriminator_loss=0.489442, adversarial_loss=1.410735\n","path:  output/epoch_0008_output.png\n","[INFO] starting epoch 9 of 20...\n","[INFO] Step 9_545: discriminator_loss=0.518218, adversarial_loss=1.703115\n","path:  output/epoch_0009_output.png\n","[INFO] starting epoch 10 of 20...\n","[INFO] Step 10_545: discriminator_loss=0.489566, adversarial_loss=1.537454\n","path:  output/epoch_0010_output.png\n","[INFO] starting epoch 11 of 20...\n","[INFO] Step 11_545: discriminator_loss=0.488950, adversarial_loss=1.403979\n","path:  output/epoch_0011_output.png\n","[INFO] starting epoch 12 of 20...\n","[INFO] Step 12_545: discriminator_loss=0.456874, adversarial_loss=1.569753\n","path:  output/epoch_0012_output.png\n","[INFO] starting epoch 13 of 20...\n","[INFO] Step 13_545: discriminator_loss=0.477996, adversarial_loss=1.470987\n","path:  output/epoch_0013_output.png\n","[INFO] starting epoch 14 of 20...\n","[INFO] Step 14_545: discriminator_loss=0.480967, adversarial_loss=1.332792\n","path:  output/epoch_0014_output.png\n","[INFO] starting epoch 15 of 20...\n","[INFO] Step 15_545: discriminator_loss=0.497135, adversarial_loss=1.423750\n","path:  output/epoch_0015_output.png\n","[INFO] starting epoch 16 of 20...\n","[INFO] Step 16_545: discriminator_loss=0.469538, adversarial_loss=1.549727\n","path:  output/epoch_0016_output.png\n","[INFO] starting epoch 17 of 20...\n","[INFO] Step 17_545: discriminator_loss=0.471259, adversarial_loss=1.458499\n","path:  output/epoch_0017_output.png\n","[INFO] starting epoch 18 of 20...\n","[INFO] Step 18_545: discriminator_loss=0.471066, adversarial_loss=1.701060\n","path:  output/epoch_0018_output.png\n","[INFO] starting epoch 19 of 20...\n","[INFO] Step 19_545: discriminator_loss=0.464028, adversarial_loss=1.675567\n","path:  output/epoch_0019_output.png\n","[INFO] starting epoch 20 of 20...\n","[INFO] Step 20_545: discriminator_loss=0.430930, adversarial_loss=1.463395\n","path:  output/epoch_0020_output.png\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XeUNDwM7dQwW","executionInfo":{"status":"ok","timestamp":1625503417370,"user_tz":-420,"elapsed":16,"user":{"displayName":"Vũ Nguyễn Thái","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gibo23x78nzU8U0Hl95G0sBjdvzzNTmI98CIlHR=s64","userId":"18002931970831166896"}}},"source":[""],"execution_count":8,"outputs":[]}]}